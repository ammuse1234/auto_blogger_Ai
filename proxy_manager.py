import requests
import random
import time
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

# âœ… Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ© Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§
EUROPEAN_COUNTRIES = {
    "fr", "de", "nl", "it", "es", "gb", "pl", "se", "fi", "no", "dk", "be", "at", "ch",
    "ie", "cz", "pt", "sk", "gr", "hu", "ro", "bg", "hr", "ee", "lt", "lv", "si", "cy", "lu"
}

# â¬‡ï¸ ØªÙˆÙ„ÙŠØ¯ Ø±Ø§Ø¨Ø· Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù…Ù† Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ©
country_param = ",".join(EUROPEAN_COUNTRIES)

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ
PROXY_SOURCES = [
    f"https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=3000&country={country_param}&ssl=all&anonymity=elite",
    "https://raw.githubusercontent.com/hendrikbgr/Free-Proxy-List/master/proxy-list.txt",
    "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTPS_RAW.txt"
    "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
    "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt",
]

BLOG_URL = "https://inspirat12.blogspot.com"

# âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ù„Ø§Øª ÙØ¹Ù„ÙŠØ© + ØªØ¬Ø±Ø¨Ø© ÙØªØ­ Ù…Ù‚Ø§Ù„Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
def is_proxy_working(proxy, timeout=8):
    proxies = {
        "http": f"http://{proxy}",
        "https": f"http://{proxy}",
    }

    try:
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: ÙØªØ­ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        response = requests.get(BLOG_URL, proxies=proxies, timeout=timeout)
        if response.status_code != 200:
            return False

        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a')

        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
        article_links = []
        for link in links:
            href = link.get('href')
            if href and href.startswith(BLOG_URL) and href.endswith(".html") and "/search" not in href:
                article_links.append(href)

        if not article_links:
            return False  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª ØµØ§Ù„Ø­Ø© Ø±ØºÙ… ÙØªØ­ Ø§Ù„ØµÙØ­Ø©

        # Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ¬Ø±Ø¨Ø© ÙØªØ­ Ø£ÙˆÙ„ Ù…Ù‚Ø§Ù„Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        test_article_url = article_links[0]
        article_response = requests.get(test_article_url, proxies=proxies, timeout=timeout)
        return article_response.status_code == 200

    except:
        return False

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ù…ØµØ¯Ø± Ù…Ø¹ÙŠÙ†
def fetch_proxies_from_source(source_url):
    try:
        response = requests.get(source_url, timeout=10)
        proxies = response.text.strip().split("\n")
        return [p.strip() for p in proxies if ":" in p]
    except:
        return []

# âœ… ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±
def fetch_all_proxies():
    all_proxies = set()
    for url in PROXY_SOURCES:
        proxies = fetch_proxies_from_source(url)
        all_proxies.update(proxies)
    return list(all_proxies)

# âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
def validate_proxies_parallel(proxy_list, max_workers=50):
    valid_proxies = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {executor.submit(is_proxy_working, proxy): proxy for proxy in proxy_list}
        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                if future.result():
                    valid_proxies.append(proxy)
            except:
                continue
    return valid_proxies

# âœ… Ø¬Ù„Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ©
def get_required_proxies(required_count=50, max_attempts=10):
    all_valid = set()
    attempt = 0

    while len(all_valid) < required_count and attempt < max_attempts:
        print(f"ğŸ”„ Attempt {attempt + 1}: Fetching new proxies...")
        raw_proxies = fetch_all_proxies()
        random.shuffle(raw_proxies)
        valid = validate_proxies_parallel(raw_proxies)
        all_valid.update(valid)
        print(f"âœ… Found {len(all_valid)} valid proxies so far.")
        attempt += 1
        time.sleep(2)

    if len(all_valid) >= required_count:
        print(f"ğŸ¯ Success! Got {required_count} proxies.")
    else:
        print(f"âš ï¸ Only got {len(all_valid)} proxies after {max_attempts} attempts.")

    return list(all_valid)[:required_count]

def quick_check(proxy, timeout=5):
    try:
        response = requests.get("https://inspirat12.blogspot.com", proxies={
            "http": f"http://{proxy}",
            "https": f"http://{proxy}"
        }, timeout=timeout)
        return response.status_code == 200
    except:
        return False
