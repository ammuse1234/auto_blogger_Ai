import requests
import random
import time
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

# âœ… Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ©
EUROPEAN_COUNTRIES = {
    "fr", "de", "nl", "it", "es", "gb", "pl", "se", "fi", "no", "dk", "be", "at", "ch",
    "ie", "cz", "pt", "sk", "gr", "hu", "ro", "bg", "hr", "ee", "lt", "lv", "si", "cy", "lu"
}
country_param = ",".join(EUROPEAN_COUNTRIES)

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ
PROXY_SOURCES = [
    f"https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=3000&country={country_param}&ssl=all&anonymity=elite",
    "https://raw.githubusercontent.com/hendrikbgr/Free-Proxy-List/master/proxy-list.txt",
    "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTPS_RAW.txt",
    "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
    "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt",
]

# âœ… Ø±Ø§Ø¨Ø· Ù…Ø¯ÙˆÙ†ØªÙƒ
BLOG_URL = "https://ammuse12345.blogspot.com"

# âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ (Ù‡Ù„ ÙŠÙØªØ­ Ø§Ù„ØµÙØ­Ø© + Ù…Ù‚Ø§Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŸ)
def is_proxy_working(proxy, timeout=8):
    proxies = {
        "http": f"http://{proxy}",
        "https": f"http://{proxy}",
    }

    try:
        response = requests.get(BLOG_URL, proxies=proxies, timeout=timeout)
        if response.status_code != 200:
            return False

        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a')
        article_links = [link.get('href') for link in links if link.get('href') and link.get('href').startswith(BLOG_URL) and link.get('href').endswith(".html") and "/search" not in link.get('href')]

        if not article_links:
            return False

        # ØªØ¬Ø±Ø¨Ø© ÙØªØ­ Ù…Ù‚Ø§Ù„Ø©
        test_article = article_links[0]
        article_response = requests.get(test_article, proxies=proxies, timeout=timeout)
        return article_response.status_code == 200

    except:
        return False

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ù…ØµØ¯Ø± Ù…Ø¹ÙŠÙ†
def fetch_proxies_from_source(source_url):
    try:
        response = requests.get(source_url, timeout=10)
        proxies = response.text.strip().split("\n")
        return [p.strip() for p in proxies if ":" in p]
    except:
        return []

# âœ… ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±
def fetch_all_proxies():
    all_proxies = set()
    for url in PROXY_SOURCES:
        proxies = fetch_proxies_from_source(url)
        all_proxies.update(proxies)
    return list(all_proxies)

# âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ IP Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ
def extract_ip_from_proxy(proxy_string):
    return proxy_string.split(":")[0].strip()

# âœ… ÙØ­Øµ Ù‡Ù„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ù…Ø´Ø¨ÙˆÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ip-api.com
def is_proxy_suspicious(proxy_ip):
    try:
        url = f"http://ip-api.com/json/{proxy_ip}?fields=status,message,query,countryCode,org,as,mobile,proxy,hosting"
        response = requests.get(url, timeout=5)
        data = response.json()

        if data["status"] != "success":
            print(f"âš ï¸ IP-API failed for {proxy_ip}: {data.get('message')}")
            return True

        if data.get("proxy") or data.get("hosting"):
            print(f"âŒ Rejected suspicious IP: {proxy_ip} ({data.get('as')})")
            return True

        return False
    except:
        return True  # Ù†Ø¹ØªØ¨Ø±Ù‡ Ù…Ø´Ø¨ÙˆÙ‡ Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ÙØ­Øµ

# âœ… ÙØ­Øµ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ + Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡
def validate_proxies_parallel(proxy_list, max_workers=50):
    valid_proxies = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {executor.submit(is_proxy_working, proxy): proxy for proxy in proxy_list}

        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                if future.result():
                    ip = extract_ip_from_proxy(proxy)
                    if not is_proxy_suspicious(ip):
                        valid_proxies.append(proxy)
                    else:
                        print(f"â›” Proxy rejected (bad reputation): {proxy}")
            except:
                continue
    return valid_proxies

# âœ… Ø¬Ù„Ø¨ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©
def get_required_proxies(required_count=50, max_attempts=10):
    all_valid = set()
    attempt = 0

    while len(all_valid) < required_count and attempt < max_attempts:
        print(f"ğŸ”„ Attempt {attempt + 1}: Fetching proxies...")
        raw_proxies = fetch_all_proxies()
        random.shuffle(raw_proxies)
        valid = validate_proxies_parallel(raw_proxies)
        all_valid.update(valid)
        print(f"âœ… Valid proxies found: {len(all_valid)}")
        attempt += 1
        time.sleep(2)

    if len(all_valid) >= required_count:
        print(f"ğŸ¯ Success! {required_count} proxies ready.")
    else:
        print(f"âš ï¸ Only {len(all_valid)} proxies after {max_attempts} attempts.")

    return list(all_valid)[:required_count]

# âœ… ÙØ­Øµ Ø³Ø±ÙŠØ¹ (Ù„Ù„Ø·ÙˆØ§Ø±Ø¦)
def quick_check(proxy, timeout=5):
    try:
        response = requests.get(BLOG_URL, proxies={
            "http": f"http://{proxy}",
            "https": f"http://{proxy}"
        }, timeout=timeout)
        return response.status_code == 200
    except:
        return False
